{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementing Transformers for Text Generation**"
      ],
      "metadata": {
        "id": "ScgoZzUTdctZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1CLGtZkdb0C"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.16.2\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up the environment**\n",
        "*   Import necessary libraries and load the data set\n",
        "*   Preprocess the data set using the TextVectorization layer to convert text into integer sequences.\n"
      ],
      "metadata": {
        "id": "ikcpeVGxd4N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.utils import get_file"
      ],
      "metadata": {
        "id": "xZ7PWx06duNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "path_to_file = get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Preview the dataset\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "5BBd0QCidxN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the dataset\n",
        "vocab_size = 10000\n",
        "seq_length = 100\n",
        "\n",
        "# Adapt TextVectorization to full text\n",
        "vectorizer = TextVectorization(max_tokens=vocab_size, output_mode='int')\n",
        "text_ds = tf.data.Dataset.from_tensor_slices([text]).batch(1)\n",
        "vectorizer.adapt(text_ds)\n",
        "\n",
        "# Vectorize the text\n",
        "vectorized_text = vectorizer([text])[0]\n",
        "print(\"Vectorized text shape:\", vectorized_text.shape)\n",
        "print(\"First 10 vectorized tokens:\", vectorized_text.numpy()[:10])"
      ],
      "metadata": {
        "id": "Smz4bC-IeOFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create input and target sequences**\n",
        "\n",
        "Generate input and target sequences for training the Transformer model.\n",
        "\n",
        "*   Define a function to generate input and target sequences.    \n",
        "*   Split the text data into sequences of the specified length.\n",
        "*   Convert the sequences into TensorFlow tensors for training.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DhbIjdz8eSLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(text, seq_length):\n",
        "    input_seqs = []\n",
        "    target_seqs = []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        input_seq = text[i:i + seq_length]\n",
        "        target_seq = text[i + 1:i + seq_length + 1]\n",
        "        input_seqs.append(input_seq)\n",
        "        target_seqs.append(target_seq)\n",
        "    return np.array(input_seqs), np.array(target_seqs)\n",
        "\n",
        "# Generate sequences\n",
        "X, Y = create_sequences(vectorized_text.numpy(), seq_length)\n",
        "\n",
        "# Check if sequences are correctly generated\n",
        "print(\"Number of sequences generated:\", len(X))\n",
        "print(\"Sample input sequence:\", X[0] if len(X) > 0 else \"No sequences generated\")\n",
        "\n",
        "# Check if X and Y are not empty\n",
        "assert X.size > 0, \"Input data X is empty\"\n",
        "assert Y.size > 0, \"Target data Y is empty\"\n",
        "X = tf.convert_to_tensor(X)\n",
        "Y = tf.convert_to_tensor(Y)\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of Y:\", Y.shape)\n"
      ],
      "metadata": {
        "id": "zjyLhWS0eprI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the Transformer model**\n",
        "\n",
        "*   Define the TransformerBlock class that includes multi-head attention and feedforward layers with normalization and dropout.\n",
        "\n",
        "*   Define the TransformerModel class, including embedding, positional encoding, and multiple Transformer blocks.\n",
        "\n",
        "*   Compile the Transformer model using the Adam optimizer and sparse categorical cross-entropy loss function."
      ],
      "metadata": {
        "id": "9aekzSUUeqpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, LayerNormalization, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TransformerModel(Model):  # Model is now properly imported\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = self.positional_encoding(seq_length, embed_dim)\n",
        "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n",
        "        self.dense = Dense(vocab_size)\n",
        "\n",
        "    def positional_encoding(self, seq_length, embed_dim):\n",
        "        angle_rads = self.get_angles(np.arange(seq_length)[:, np.newaxis], np.arange(embed_dim)[np.newaxis, :], embed_dim)\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def get_angles(self, pos, i, embed_dim):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        x = self.embedding(inputs)\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)  # Pass training argument correctly\n",
        "        output = self.dense(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "xe2PDdite6kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embed_dim = 256\n",
        "num_heads = 4\n",
        "ff_dim = 512\n",
        "num_layers = 4\n",
        "\n",
        "# Build the Transformer model\n",
        "model = TransformerModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length)\n",
        "\n",
        "# Provide input shape to build the model by passing a dummy input with maxval specified\n",
        "_ = model(tf.random.uniform((1, seq_length), maxval=vocab_size, dtype=tf.int32))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "KYAy9jWne8tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Transformer model**\n",
        "*   Train the Transformer model on the input and target sequences\n",
        "*   Plot the training loss to monitor the model's performance over epochs\n"
      ],
      "metadata": {
        "id": "NSeOgssXfAl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for training visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Early stopping callback to stop training if the loss doesn't improve\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "# Train the transformer model on the full input and target sequences\n",
        "history = model.fit(X, Y, epochs=20, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Plot training loss to monitor model performance over epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Ynf-qsWe-4H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}